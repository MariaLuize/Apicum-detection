{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bibliotecas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os, logging, ee, folium, glob, rasterio\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import tensorflow as tf\n",
    "import json, datetime, math\n",
    "import numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from rasterio.plot import show\n",
    "from osgeo import ogr, gdal\n",
    "\n",
    "\n",
    "\n",
    "logging.getLogger('googleapicliet.discovery_cache').setLevel(logging.ERROR)\n",
    "GPU_AFFINTY  = 0\n",
    "GPU_MEMORY_LIMIT_GB = 10\n",
    "\n",
    "gpu_dict = {'4090':{'GPU_AFFINTY' : 0, 'GPU_MEMORY_LIMIT_GB':12}, \n",
    "            '2070':{'GPU_AFFINTY':1, 'GPU_MEMORY_LIMIT_GB':8}}\n",
    "\n",
    "sel_gpu = '4090'\n",
    "GPU_AFFINTY  = gpu_dict[sel_gpu]['GPU_AFFINTY']\n",
    "GPU_MEMORY_LIMIT_GB =gpu_dict[sel_gpu]['GPU_MEMORY_LIMIT_GB']\n",
    "\n",
    "\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# GPU config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.15.0\n",
      "Folium Version: 0.14.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "\n",
    "print('Tensorflow Version:',tf.__version__)\n",
    "print('Folium Version:',folium.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[GPU_AFFINTY], 'GPU')\n",
    "    GPU_MEMORY_LIMIT_GB = GPU_MEMORY_LIMIT_GB * 1e3\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    if GPU_MEMORY_LIMIT_GB == 0:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    else:\n",
    "        tf.config.set_logical_device_configuration(gpus[GPU_AFFINTY],[tf.config.LogicalDeviceConfiguration(memory_limit=GPU_MEMORY_LIMIT_GB)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def creatDirectory(new_folder):\n",
    "    if not os.path.exists(new_folder):\n",
    "        print(f'lets make the directory: {new_folder}')\n",
    "        os.makedirs(new_folder)\n",
    "    else: return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# ENV Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['swir1', 'nir', 'red', 'green', 'MNDWI', 'NDVI']\n"
     ]
    }
   ],
   "source": [
    "# General \n",
    "GPU_AFFINITY   = gpus[GPU_AFFINTY].name\n",
    "VERSION        = 'DATA_AUGMENTATION'\n",
    "# VERSION        = '4_p5_WO_DATA_AUGMENTATION'\n",
    "MOSAIC_VERSION = '1'\n",
    "GOAL_CLASS     = 'apicum'\n",
    "GOAL_YEAR      = '2022'\n",
    "\n",
    "FOLDER_TRAIN   = 'training_samples'\n",
    "FOLDER_EVAL    = 'eval_samples'\n",
    "TRAINING_BASE  = 'training_patches_'+ MOSAIC_VERSION\n",
    "EVAL_BASE      = 'eval_patches_'+ MOSAIC_VERSION\n",
    "\n",
    "#Local paths\n",
    "LOCAL_PATH  = '~/local_path/apicum_segmentation'\n",
    "MODEL_DIR   = LOCAL_PATH+'/checkpoint/v'+VERSION\n",
    "OUTPUT_PATH = LOCAL_PATH+'/output/v'+VERSION\n",
    "creatDirectory(MODEL_DIR)\n",
    "creatDirectory(OUTPUT_PATH)\n",
    "\n",
    "# Exportation Configs\n",
    "FOLDER_patch = 'allPatch'\n",
    "\n",
    "# Specify inputs (Landsat bands) to the model and the response variable.\n",
    "opticalBands = ['swir1', 'nir', 'red','green','MNDWI','NDVI'] \n",
    "\n",
    "# FOR DEEPLABV3\n",
    "# opticalBands = ['red','green','blue'] #['swir1', 'nir', 'red','NDVI','MNDWI'] DEFAULT\n",
    "BANDS        = opticalBands\n",
    "RESPONSE     = 'supervised'\n",
    "FEATURES     = BANDS + [RESPONSE]\n",
    "\n",
    "# Specify the size and shape of patches expected by the model.\n",
    "KERNEL_SIZE = 256\n",
    "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
    "COLUMNS = [\n",
    "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
    "]\n",
    "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
    "\n",
    "# Sizes of the training and evaluation datasets.\n",
    "TRAIN_SIZE = 0\n",
    "EVAL_SIZE  = 0\n",
    "\n",
    "# Specify model training parameters.\n",
    "BATCH_SIZE  = 10 \n",
    "DROPOUT     = 0.3\n",
    "EPOCHS      = 50\n",
    "BUFFER_SIZE = 1000 \n",
    "OPTIMIZER   = 'Nadam' \n",
    "LOSS        = 'BinaryCrossentropy'\n",
    "METRICS     = ['RootMeanSquaredError']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseClassV       = '4'\n",
    "yearClass_class  = '2022'\n",
    "yearClass_mosaic = '2022'\n",
    "version_final    = '4'\n",
    "classID          = 32\n",
    "\n",
    "supervised_layer  = ee.Image('projects/apicum-segmentation/assets/supervisedImage_unet_apicum_2022_final').eq(classID).rename(RESPONSE);\n",
    "supervisedChannel = supervised_layer.toByte().rename(RESPONSE);\n",
    "\n",
    "'''  DEFAULT '''\n",
    "image = ee.Image('projects/mapbiomas-workspace/TRANSVERSAIS/ZONACOSTEIRA6/mosaic_'+yearClass_mosaic).addBands(supervisedChannel)\n",
    "mapid = image.getMapId({'bands': ['swir1', 'nir', 'red'], 'min': 30, 'max': 150})\n",
    "\n",
    "# '''  FOR DEEPLABV3 '''\n",
    "# image = ee.Image('projects/mapbiomas-workspace/TRANSVERSAIS/ZONACOSTEIRA6/mosaic_'+yearClass_mosaic+'_with_blue').addBands(supervisedChannel)\n",
    "# mapid = image.getMapId({'bands': ['red', 'green', 'blue'], 'min': 11, 'max': 95})\n",
    "\n",
    "map = folium.Map(location=[-23.0089, -43.6078],zoom_start=13)\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Planet',\n",
    "    overlay=True,\n",
    "    name='Mosaic composite',\n",
    "  ).add_to(map)\n",
    "mapid = supervisedChannel.select(RESPONSE).mask(supervisedChannel.eq(1)).getMapId({'min': 0, 'max': 1, 'palette':'red'})\n",
    "# map = folium.Map(location=[-23.0089, -43.6078])\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='Apicum '+yearClass_class,\n",
    "  ).add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "featureStack = ee.Image.cat([\n",
    "  image.select(BANDS).unmask(0),\n",
    "  image.select(RESPONSE).unmask(0)\n",
    "]).float()\n",
    "\n",
    "list = ee.List.repeat(1, KERNEL_SIZE)\n",
    "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
    "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
    "\n",
    "arrays = featureStack.neighborhoodToArray(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "yearClass_geoms = '2020'\n",
    "trainingPolys_v1 = ee.FeatureCollection('projects/apicum-segmentation/assets/Geometries/trainPolys_apicum_p1')\n",
    "evalPolys_v1     = ee.FeatureCollection('projects/apicum-segmentation/assets/Geometries/testPolys_apicum_p1')\n",
    "\n",
    "trainingPolys_v2 = ee.FeatureCollection('projects/apicum-segmentation/assets/Geometries/trainPolys_apicum_p2')\n",
    "evalPolys_v2     = ee.FeatureCollection('projects/apicum-segmentation/assets/Geometries/testPolys_apicum_p2')\n",
    "\n",
    "trainingPolys_v3 = ee.FeatureCollection('projects/apicum-segmentation/assets/Geometries/trainPolys_apicum_p3')\n",
    "evalPolys_v3     = ee.FeatureCollection('projects/apicum-segmentation/assets/Geometries/testPolys_apicum_p3')\n",
    "\n",
    "trainingPolys = trainingPolys_v1.merge(trainingPolys_v2)\n",
    "evalPolys     = evalPolys_v1.merge(evalPolys_v2).merge(evalPolys_v3)\n",
    "\n",
    "id_filter_out = ['2_00000000000000000018','2_0000000000000000001f','1_00000000000000000004','2_0000000000000000001d']\n",
    "trainingPolys = trainingPolys.filter(ee.Filter.inList('system:index', id_filter_out).Not())\n",
    "\n",
    "trainingPolys = trainingPolys.merge(trainingPolys_v3)\n",
    "\n",
    "def geo_type(feature):\n",
    "    return feature.set('geo_type', feature.geometry().type())\n",
    "trainingPolys = trainingPolys.map(lambda feat: geo_type(feat))\n",
    "trainingPolys = trainingPolys.filter(ee.Filter.neq('geo_type','LineString'))\n",
    "\n",
    "print(trainingPolys.size().getInfo())\n",
    "print(evalPolys.size().getInfo())\n",
    "\n",
    "polyImage = ee.Image(0).byte().paint(trainingPolys, 1).paint(evalPolys, 2)\n",
    "polyImage = polyImage.updateMask(polyImage)\n",
    "\n",
    "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['red', 'blue']})\n",
    "map = folium.Map(location=[-1.3621, -45.2738], zoom_start=5)\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='training polygons',\n",
    "  ).add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "# map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Train/Validation Sampling Exportation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:31800\n",
      "EVAL:13400\n"
     ]
    }
   ],
   "source": [
    "# Convert the feature collections to lists for iteration.\n",
    "trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
    "evalPolysList = evalPolys.toList(evalPolys.size())\n",
    "# These numbers determined experimentally.\n",
    "n = 20 # Number of shards in each polygon.\n",
    "N = 200 # Total sample size in each polygon.\n",
    "\n",
    "version_samples_acc = \"final\"\n",
    "\n",
    "#Add some generalism\n",
    "TRAIN_SIZE = trainingPolys.size().getInfo()*N\n",
    "EVAL_SIZE = evalPolys.size().getInfo()*N\n",
    "print('TRAIN:'+str(TRAIN_SIZE))\n",
    "print('EVAL:'+str(EVAL_SIZE))\n",
    "GDRIVE = 'Apicum_segmentation'\n",
    "# Export all the training data (in many pieces), with one task \n",
    "# per geometry.\n",
    "for g in range(trainingPolys.size().getInfo()):\n",
    "  geomSample = ee.FeatureCollection([])\n",
    "  for i in range(n):\n",
    "    sample = arrays.sample(\n",
    "      region = ee.Feature(trainingPolysList.get(g)).geometry(), \n",
    "      scale = 30, \n",
    "      numPixels = N / n, # Size of the shard.\n",
    "      seed = i,\n",
    "      tileScale = 8\n",
    "    )\n",
    "    geomSample = geomSample.merge(sample)\n",
    "  \n",
    "  desc = TRAINING_BASE + '_g' + str(g)\n",
    "  task = ee.batch.Export.table.toDrive(\n",
    "    collection = geomSample,\n",
    "    description = desc, \n",
    "    folder = GDRIVE+'/'+FOLDER_TRAIN+'_v'+version_samples_acc, \n",
    "    fileNamePrefix = desc,\n",
    "    fileFormat = 'TFRecord',\n",
    "    selectors = BANDS + [RESPONSE]\n",
    "  )\n",
    "  task.start()\n",
    "\n",
    "# Export all the evaluation data.\n",
    "for g in range(evalPolys.size().getInfo()):\n",
    "  geomSample = ee.FeatureCollection([])\n",
    "  for i in range(n):\n",
    "    sample = arrays.sample(\n",
    "      region = ee.Feature(evalPolysList.get(g)).geometry(), \n",
    "      scale = 30, \n",
    "      numPixels = N / n,\n",
    "      seed = i,\n",
    "      tileScale = 8\n",
    "    )\n",
    "    geomSample = geomSample.merge(sample)\n",
    "  \n",
    "  desc = EVAL_BASE + '_g' + str(g)\n",
    "  task = ee.batch.Export.table.toDrive(\n",
    "    collection = geomSample,\n",
    "    description = desc, \n",
    "    folder = GDRIVE+'/'+FOLDER_EVAL+version_samples_acc, \n",
    "    fileNamePrefix = desc,\n",
    "    fileFormat = 'TFRecord',\n",
    "    selectors = BANDS + [RESPONSE],\n",
    "  )\n",
    "  task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Datasets auxiliary reading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_tfrecord(example_proto):\n",
    "  \"\"\"The parsing function.\n",
    "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
    "  Args:\n",
    "    example_proto: a serialized Example.\n",
    "  Returns: \n",
    "    A dictionary of tensors, keyed by feature name.\n",
    "  \"\"\"\n",
    "  print(FEATURES_DICT)\n",
    "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
    "\n",
    "\n",
    "\n",
    "def to_tuple(inputs):\n",
    "  \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
    "  Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
    "  Args:\n",
    "    inputs: A dictionary of tensors, keyed by feature name.\n",
    "  Returns: \n",
    "    A dtuple of (inputs, outputs).\n",
    "  \"\"\"\n",
    "  inputsList = [inputs.get(key) for key in FEATURES]\n",
    "  stacked = tf.stack(inputsList, axis=0)\n",
    "  # Convert from CHW to HWC\n",
    "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
    "\n",
    "\n",
    "def get_dataset(pattern):\n",
    "  \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
    "  Get all the files matching the pattern, parse and convert to tuple.\n",
    "  Args:\n",
    "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
    "  Returns: \n",
    "    A tf.data.Dataset\n",
    "  \"\"\"\n",
    "  # glob = tf.gfile.Glob(pattern) for tendorflow 1.x\n",
    "  glob = tf.io.gfile.glob(pattern) # for tendorflow 2.x\n",
    "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
    "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Read and mount train/validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'swir1': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None), 'nir': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None), 'red': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None), 'green': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None), 'MNDWI': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None), 'NDVI': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None), 'supervised': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None)}\n",
      "{'swir1': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None), 'nir': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None), 'red': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None), 'green': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None), 'MNDWI': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None), 'NDVI': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None), 'supervised': FixedLenFeature(shape=[256, 256], dtype=tf.float32, default_value=None)}\n",
      "\n",
      "\n",
      "<_TakeDataset element_spec=(TensorSpec(shape=(None, 256, 256, 6), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256, 256, 1), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "import copy\n",
    "import random\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "data_augmentation_model = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"vertical\", seed=seed),\n",
    "    layers.RandomRotation(factor=0.2, seed=seed),\n",
    "    layers.RandomTranslation(height_factor=0.1, width_factor=0.1, seed=seed)\n",
    "])\n",
    "\n",
    "def create_data_augmentation_v2(model):\n",
    "\n",
    "    return model\n",
    "\n",
    "def augment_fn(x, y):\n",
    "    # data_augmentation_func = create_data_augmentation(seed)\n",
    "    data_augmentation_func = create_data_augmentation_v2(data_augmentation_model)\n",
    "    input_data = tf.concat([x, y], axis=-1)\n",
    "    \n",
    "    # Check if the instance contains your target class\n",
    "    tensor_sum_value  = tf.math.reduce_sum(y, axis=[0,1,2])\n",
    "    tensor_base_value = tf.constant([183.], dtype=tf.float32)\n",
    "    greater_tensor    = tf.greater(tensor_sum_value, tensor_base_value)\n",
    "\n",
    "    if tf.reduce_any(greater_tensor):\n",
    "        augmented_data = data_augmentation_func(input_data)\n",
    "        x_aug = augmented_data[:, :, :len(BANDS)]\n",
    "        y_aug = augmented_data[:, :, len(BANDS):]\n",
    "        return (x_aug, y_aug)\n",
    "    else:\n",
    "        return (x, y)\n",
    "    \n",
    "def is_augmented(x, y):\n",
    "    tensor_sum_value  = tf.math.reduce_sum(y, axis=[0,1,2])\n",
    "    tensor_base_value = tf.constant([183.], dtype=tf.float32)\n",
    "    greater_tensor    = tf.greater(tensor_sum_value, tensor_base_value)\n",
    "    return tf.reduce_any(greater_tensor)\n",
    "\n",
    "\n",
    "def get_training_dataset_data_aug_target(SAMPLE_PATH):\n",
    "    \"\"\"Get the preprocessed training dataset\n",
    "  Returns: \n",
    "    A tf.data.Dataset of training data.\n",
    "  \"\"\"\n",
    "    \n",
    "    glob    = SAMPLE_PATH + '/'+ TRAINING_BASE + '*'   \n",
    "    dataset_bkp = get_dataset(glob)\n",
    "    dataset     = get_dataset(glob)\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "    num_samples = 31800\n",
    "    \n",
    "    for index in range(1):\n",
    "        augmented_dataset = dataset_bkp.map(augment_fn)\n",
    "        augmented_only_dataset = augmented_dataset.filter(is_augmented).take(15900)\n",
    "        dataset = dataset.concatenate(augmented_only_dataset)\n",
    "    \n",
    "    # dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "    # train_da_size = dataset.reduce(np.int64(0), lambda x,_ : x + 1).numpy()\n",
    "    # print(train_da_size) #46767\n",
    "    # 250 => 46767\n",
    "    # 183 => 47701\n",
    "    # 182 => 47732\n",
    "    # 180 => 47750\n",
    "    # 175 => 47854\n",
    "    # 170 => 47931\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE, reshuffle_each_iteration=True).batch(12).repeat().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "TRAIN_PATH = '~/local_path/apicum_segmentation/train/samples_v'+version_samples_acc\n",
    "creatDirectory(TRAIN_PATH)\n",
    "training = get_training_dataset_data_aug_target(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_eval_dataset(SAMPLE_PATH):\n",
    "    glob    = SAMPLE_PATH + '/'+ EVAL_BASE + '*'\n",
    "    dataset = get_dataset(glob)\n",
    "    # eval_size = dataset.reduce(np.int64(0), lambda x,_ : x + 1).numpy()\n",
    "    # print(eval_size)  #13355\n",
    "    dataset = dataset.batch(1).repeat()  \n",
    "    # return dataset, eval_size\n",
    "    return dataset\n",
    "\n",
    "EVAL_PATH = '~/local_path/apicum_segmentation/eval/samples_v'+version_samples_acc\n",
    "creatDirectory(EVAL_PATH)\n",
    "evaluation = get_eval_dataset(EVAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# U-shaped model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "def conv_block(input_tensor, num_filters):\n",
    "    encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "    encoder = layers.BatchNormalization()(encoder)\n",
    "    encoder = layers.Activation('relu')(encoder)\n",
    "    encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
    "    encoder = layers.BatchNormalization()(encoder)\n",
    "    encoder = layers.Activation('relu')(encoder)\n",
    "    return encoder\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "    encoder = conv_block(input_tensor, num_filters)\n",
    "    encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "    return encoder_pool, encoder\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "    decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "    decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
    "    decoder = layers.BatchNormalization()(decoder)\n",
    "    decoder = layers.Activation('relu')(decoder)\n",
    "    decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "    decoder = layers.BatchNormalization()(decoder)\n",
    "    decoder = layers.Activation('relu')(decoder)\n",
    "    decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "    decoder = layers.BatchNormalization()(decoder)\n",
    "    decoder = layers.Activation('relu')(decoder)\n",
    "    return decoder\n",
    "\n",
    "def get_model():\n",
    "    inputs = layers.Input(shape=[None, None, len(BANDS)]) # 256\n",
    "    encoder0_pool, encoder0 = encoder_block(inputs, 64) # 128\n",
    "    encoder1_pool, encoder1 = encoder_block(encoder0_pool, 128) # 64\n",
    "    encoder2_pool, encoder2 = encoder_block(encoder1_pool, 256) # 32\n",
    "    encoder3_pool, encoder3 = encoder_block(encoder2_pool, 512) # 16\n",
    "    center = conv_block(encoder3_pool, 1024) # 8 center\n",
    "    decoder4 = decoder_block(center, encoder3, 512) # 16\n",
    "    decoder3 = decoder_block(decoder4, encoder2, 256) # 32\n",
    "    decoder2 = decoder_block(decoder3, encoder1, 128) # 64\n",
    "    decoder1 = decoder_block(decoder2, encoder0, 64) # 128\n",
    "    dropout = layers.Dropout(DROPOUT, name=\"dropout\", noise_shape=None, seed=None)(decoder1)\n",
    "    outputs = layers.Conv2D(1, (1, 1),  activation=tf.nn.sigmoid, padding='same', kernel_initializer=tf.keras.initializers.GlorotNormal())(dropout)\n",
    "    \n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    optimizer = tf.keras.optimizers.Nadam(0.000005, name='optimizer')# LR DEFAULT\n",
    "    # optimizer = tf.keras.optimizers.Nadam(0.00001, name='optimizer')# LR 1e-5\n",
    "    \n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss=losses.get(LOSS),\n",
    "        metrics=[metrics.get(metric) for metric in METRICS]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def previewClass(epoch,log):\n",
    "    counter = 0\n",
    "    for batch in evaluation.shuffle(1000).take(3):\n",
    "        pureImage = batch[0]\n",
    "        supervised = batch[1]\n",
    "        stacked = tf.transpose(pureImage[0], [0, 1, 2]).numpy()\n",
    "        stackedS = tf.transpose(supervised[0], [0, 1, 2]).numpy()\n",
    "        test_pred_raw = m.predict(pureImage)\n",
    "        test_pred_raw = tf.transpose(test_pred_raw[0],[0, 1, 2]).numpy()\n",
    "        fig = plt.figure(figsize=[12,4])\n",
    "        # show original image\n",
    "        fig.add_subplot(131)\n",
    "        plt.imshow(stacked[:,:,0:3].astype(np.uint8), interpolation='nearest', vmin=0, vmax=255)\n",
    "        fig.add_subplot(132)\n",
    "        plt.imshow(stackedS[:,:,0], interpolation='nearest',cmap=\"gray\")\n",
    "        fig.add_subplot(133)\n",
    "        plt.imshow(test_pred_raw[:,:,0], interpolation='nearest',cmap=\"gray\")\n",
    "        plt.show()\n",
    "        counter = counter+1\n",
    "# previewClass(1,1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Multiclass semantic segmentation using DeepLabV3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' \n",
    "https://keras.io/examples/vision/deeplabv3_plus/\n",
    "https://arxiv.org/pdf/1802.02611.pdf\n",
    "\n",
    "https://keras.io/examples/vision/deeplabv3_plus/ AND https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/deeplabv3_plus.ipynb\n",
    "https://github.com/lattice-ai/DeepLabV3-Plus/tree/master\n",
    "\n",
    "DeepLabv3+ extends DeepLabv3 by adding an encoder-decoder structure. \n",
    "The encoder module processes multiscale contextual information by applying dilated convolution at multiple scales, \n",
    "while the decoder module refines the segmentation results along object boundaries.\n",
    "\n",
    "Deep Lab V3 + extends the architecture of Deep Lab V3 and Deep Lab V2. It utilises the power of Atrous Convolution and Spatial Pyramid Pooling.\n",
    "\n",
    "It combines the Atrous Convolution and Spatial Pyramid Pooling(ASPP) \n",
    "and forms the Atrous Spatial Pyramid Pooling. Which allows to extract features at multiple scales. \n",
    "Instead of using a cascade or a parallel method, it uses an Encoder-Decoder architecture.\n",
    "\n",
    "In which the Encoder learns from the high level features of ResNet50 using ASPP, Lower Level Features(LLF) of ResNet 50 are also extracted. \n",
    "This is because the LLF and the High Level features encodes different information and \n",
    "the geometric information about the object is extracted from the LLF, whereas the information about the object class is extracted from the HLF.\n",
    "\n",
    "ResNet-101 [25] as network backbone in the proposed DeepLabv3+ model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Deep lab v3 with water bodie examples:  https://www.kaggle.com/code/utkarshsaxenadn/water-bodies-segmentation-deeplabv3#Data-Loading\n",
    "\n",
    "'''\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses, metrics, optimizers\n",
    "\n",
    "from keras.layers import Input\n",
    "\n",
    "# Pretrained Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "\n",
    "def conv_block_dlv3(input_tensor, num_filters=256, kernel_size=3, dilation_rate=1, padding=\"same\", use_bias=False):\n",
    "    encoder = layers.Conv2D(\n",
    "                num_filters,\n",
    "                kernel_size        = kernel_size,\n",
    "                dilation_rate      = dilation_rate,\n",
    "                padding            = \"same\",\n",
    "                use_bias           = use_bias,\n",
    "                kernel_initializer = keras.initializers.HeNormal())(input_tensor)\n",
    "    encoder = layers.BatchNormalization()(encoder)\n",
    "    encoder = layers.Activation('relu')(encoder)\n",
    "    return encoder\n",
    "\n",
    "def DilatedSpatialPyramidPooling_dlv3(dspp_input):\n",
    "    dims = dspp_input.shape #  B, H, W, C\n",
    "    print(dims)\n",
    "    \n",
    "    # Image Pooling\n",
    "    x    = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input) # dims[-3] == height(H) and dims[-2](W) width\n",
    "    x    = conv_block_dlv3(x, kernel_size=1, use_bias=True)\n",
    "    out_pool = layers.UpSampling2D(\n",
    "        size =(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\", name=\"ASPP-ImagePool-UpSample\",\n",
    "    )(x)\n",
    "\n",
    "    # Atrous Oprtations\n",
    "    out_1  = conv_block_dlv3(dspp_input, kernel_size=1, dilation_rate=1)\n",
    "    out_6  = conv_block_dlv3(dspp_input, kernel_size=3, dilation_rate=6)\n",
    "    out_12 = conv_block_dlv3(dspp_input, kernel_size=3, dilation_rate=12)\n",
    "    out_18 = conv_block_dlv3(dspp_input, kernel_size=3, dilation_rate=18)\n",
    "\n",
    "    # Combine All\n",
    "    x      = layers.Concatenate(axis=-1, name=\"ASPP-Combine\")([out_pool, out_1, out_6, out_12, out_18])\n",
    "    output = conv_block_dlv3(x, kernel_size=1)\n",
    "    \n",
    "    # Final Output\n",
    "    return output\n",
    "\n",
    "def DeeplabV3Plus(IMAGE_SIZE, num_classes):\n",
    "    ''' ENCODER PHASE '''\n",
    "    # Input\n",
    "    # model_input = keras.Input(shape=(image_size, image_size, 3)) # FROM  https://keras.io/examples/vision/deeplabv3_plus/ AND https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/deeplabv3_plus.ipynb\n",
    "    model_input = Input(shape=[IMAGE_SIZE, IMAGE_SIZE, len(BANDS)]) # 256 (shape=[256, 256, len(BANDS)])\n",
    "    \n",
    "    # Base Mode\n",
    "    resnet50 = keras.applications.ResNet50(\n",
    "        weights = \"imagenet\", include_top=False, input_tensor=model_input\n",
    "    )\n",
    "    \n",
    "    # Atrous Spatial Pyramid Pooling(ASPP) Phase\n",
    "    DCNN = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
    "    ASPP = DilatedSpatialPyramidPooling_dlv3(DCNN)\n",
    "\n",
    "    \n",
    "    ''' DECODER PHASE '''\n",
    "    # Output from ASPP(ENCODER PHASE) with Upsample by 4 \n",
    "    ASPP = layers.UpSampling2D(\n",
    "        size = (IMAGE_SIZE // 4 // ASPP.shape[1], IMAGE_SIZE // 4 // ASPP.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(ASPP)\n",
    "    \n",
    "    # Lower Level Features(LLF) Phase\n",
    "    LLF = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "    LLF = conv_block_dlv3(LLF, num_filters=48, kernel_size=1)\n",
    "\n",
    "    # Combined\n",
    "    combined = layers.Concatenate(axis=-1)([ASPP, LLF])\n",
    "    features = conv_block_dlv3(combined)\n",
    "    features = conv_block_dlv3(features)\n",
    "    \n",
    "    # Upsample by 4\n",
    "    upsample = layers.UpSampling2D(\n",
    "        size=(IMAGE_SIZE // features.shape[1], IMAGE_SIZE // features.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(features)\n",
    "    \n",
    "    # Output Mask\n",
    "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(upsample)\n",
    "    # return keras.Model(inputs=model_input, outputs=model_output)\n",
    "    \n",
    "    model = keras.Model(inputs=model_input, outputs=model_output)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Nadam(0.000005, name='optimizer')\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss=losses.get(LOSS),\n",
    "        metrics=[metrics.get(metric) for metric in METRICS]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# # DeelLabV3+ Model\n",
    "# model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model Selection/Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "m = get_model()\n",
    "# m = DeeplabV3Plus(IMAGE_SIZE=256, num_classes=1)# DeeplabV3+\n",
    "\n",
    "# MODEL_VERSION = 'v1_DeepLabV3Plus'\n",
    "# ESCOLHIDOS PARA PREDICAO\n",
    "\n",
    "EPOCH           = 0\n",
    "MODEL_VERSION   = 'final'\n",
    "CHECK_MODEL_DIR = MODEL_DIR + '/' +MODEL_VERSION\n",
    "creatDirectory(CHECK_MODEL_DIR)\n",
    "CHECK_MODEL_DIR = CHECK_MODEL_DIR + '/cp-0'+str(EPOCH)+'.ckpt'\n",
    "# m.load_weights(CHECK_MODEL_DIR)\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime, os\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "checkpoint_path = CHECK_MODEL_DIR+\"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir  = os.path.dirname(checkpoint_path)\n",
    "n = 20 # Number of shards in each polygon.\n",
    "N = 200 # Total sample size in each polygon.\n",
    "\n",
    "log_dir = f'~/local_path/apicum_segmentation/output/v{VERSION}/{MODEL_VERSION}'\n",
    "creatDirectory(log_dir)\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir+'/log_model',write_images=True)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,verbose=1, save_weights_only=True, period=5)\n",
    "img_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=previewClass)\n",
    "\n",
    "\n",
    "m.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "TRAIN_SIZE = 47700 # with data augmentation\n",
    "# TRAIN_SIZE = 31800\n",
    "EVAL_SIZE  = 13355\n",
    "BATCH_SIZE = 12\n",
    "print(int(TRAIN_SIZE / BATCH_SIZE))\n",
    "\n",
    "result = m.fit(x=training,\n",
    "  epochs=100,\n",
    "  initial_epoch=0, # REMEBER TO CHANGE THIS INITIAL EPOCH PARAM, WHEN OTHER MODEL HAS BEEN LOADED\n",
    "  steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE), \n",
    "  verbose=1,\n",
    "  shuffle=True,\n",
    "  validation_data=evaluation,\n",
    "  validation_steps=EVAL_SIZE,\n",
    "  callbacks = [cp_callback,tensorboard, img_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pygeoj\n",
    "kernel_buffer   = [256, 256]\n",
    "image_base_name = 'allPatch_UNET_grid_'\n",
    "grid            = pygeoj.load('~/local_path/apicum_segmentation/GRID-ALLCALSSES-COL8-19052023.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Export mosaics to GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def doExport(out_image_base,index_in, kernel_buffer, roi):\n",
    "  \"\"\"Run the image export task. \n",
    "  \"\"\"\n",
    "  index = index_in\n",
    "  image = ee.Image('projects/mapbiomas-workspace/TRANSVERSAIS/ZONACOSTEIRA6/mosaic_'+str(index))\n",
    "  # Export the image, specifying scale and region.\n",
    "  task = ee.batch.Export.image.toDrive(\n",
    "    image          = image.select(BANDS).toFloat(),\n",
    "    description    = out_image_base+'_'+str(index),\n",
    "    fileNamePrefix = out_image_base+'_'+str(index), \n",
    "    folder         = 'mosaics_landsat/'+str(index),\n",
    "    scale          = 30,\n",
    "    region         = roi,\n",
    "    fileFormat     = 'TFRecord',\n",
    "    formatOptions  = { \n",
    "      'patchDimensions': KERNEL_SHAPE,\n",
    "      'kernelSize': kernel_buffer,\n",
    "      'compressed': True,\n",
    "      'maxFileSize': 157286400\n",
    "    }\n",
    "  )\n",
    "  task.start()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the export.\n",
    "for region in grid:\n",
    "    region_id = int(region.properties['id'])\n",
    "    if int(region_id) and region.properties['apicum'] == 1:\n",
    "      print('Region:',int(region_id))\n",
    "      for y in range(1985, 2021): \n",
    "          doExport(image_base_name+str(region_id)+str('_' + MOSAIC_VERSION),y, kernel_buffer, region.geometry.coordinates[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Mosaic prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def doPrediction(out_image_base,index_in,region_id, kernel_buffer, region, epochs):\n",
    "  \"\"\"Perform inference on exported imagery, upload to Earth Engine.\n",
    "  \"\"\"\n",
    "  from PIL import Image\n",
    "  import glob\n",
    "  import rasterio\n",
    "  from rasterio.plot import show\n",
    "  import numpy\n",
    "  import matplotlib.pyplot as plt\n",
    "  from osgeo import ogr\n",
    "  from osgeo import gdal\n",
    "  import os\n",
    "  import json\n",
    "  \n",
    "  \n",
    "  out_image_base = out_image_base+'_'+str(index_in)\n",
    "  \n",
    "  filesList = glob.glob(\"~/local_path/apicum_segmentation/mosaics_landsat/\"+str(index_in)+\"/\"+out_image_base+\"*\")\n",
    "  rasterFolder = f'{OUTPUT_PATH}/{MODEL_VERSION}/{epochs}epochs/classifications_tiff/{index_in}'\n",
    "  # creatDirectory(rasterFolder)\n",
    "  rasterURILZW = rasterFolder + '/outimage_'+VERSION+'_'+MODEL_VERSION+'_'+str(region_id)+'_'+str(index_in)+'_'+str(epochs)+'epochs_lzw.tif'\n",
    "\n",
    "  if os.path.exists(rasterURILZW):\n",
    "        print('File already predicted \\n\\n')\n",
    "        return None\n",
    "\n",
    "  \n",
    "  if(len(filesList) == 0):\n",
    "    print('No files')\n",
    "    !echo 'ERROR! (No files) grid={y} - {region_id}' >> log.log\n",
    "    return None\n",
    "  exportFilesList = [s for s in filesList if out_image_base in s]    \n",
    "    \n",
    "\n",
    "  # Get the list of image files and the JSON mixer file.\n",
    "  imageFilesList = []\n",
    "  jsonFile = None\n",
    "  for f in exportFilesList:\n",
    "    if f.endswith('.tfrecord.gz'):\n",
    "      imageFilesList.append(f)\n",
    "    elif f.endswith('.json'):\n",
    "      jsonFile = f\n",
    "\n",
    "  # Make sure the files are in the right order.\n",
    "  imageFilesList.sort() \n",
    "  # Load the contents of the mixer file to a JSON object.\n",
    "  jsonText = open(jsonFile)\n",
    "  # Get a single string w/ newlines from the IPython.utils.text.SList\n",
    "  mixer = json.load(jsonText)\n",
    "  \n",
    "  patches = mixer['totalPatches']\n",
    "  cols = int(mixer[\"patchesPerRow\"])\n",
    "  rows = int(mixer[\"totalPatches\"]/cols)\n",
    "  \n",
    "\n",
    "  # Get set up for prediction.\n",
    "  x_buffer = int(kernel_buffer[0] / 2)\n",
    "  y_buffer = int(kernel_buffer[1] / 2)\n",
    "  #print(\"buffer Size\",x_buffer,y_buffer)\n",
    "  buffered_shape = [\n",
    "      KERNEL_SHAPE[0] + kernel_buffer[0],\n",
    "      KERNEL_SHAPE[1] + kernel_buffer[1]]\n",
    "\n",
    "  imageColumns = [\n",
    "    tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32) #Tensorflow 2.x\n",
    "      for k in BANDS\n",
    "  ]  \n",
    "    \n",
    "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
    "  def parse_image(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, imageFeaturesDict) #Tensorflow 2.x\n",
    "\n",
    "  def toTupleImage(inputs):\n",
    "    inputsList = [inputs[key] for key in BANDS] #BANDS\n",
    "    stacked = tf.stack(inputsList, axis=0)\n",
    "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "    return stacked\n",
    "  \n",
    "  # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
    "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
    "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=4)\n",
    "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
    "    \n",
    "    \n",
    "  # Perform inference.\n",
    "  predictions = m.predict(imageDataset, steps=patches, verbose=2)\n",
    "  patchesPerRow  = mixer['patchesPerRow']\n",
    "  TotalPatches   = mixer['totalPatches']\n",
    "  patchDimension = mixer['patchDimensions']\n",
    "\n",
    "  #Manipulating Prediction Numpy Array OUTPUT\n",
    "  counter       = 1\n",
    "  rowCounter    = 1\n",
    "  globalCounter = 0\n",
    "  finalArray    = numpy.array([])\n",
    "\n",
    "  rowArray = numpy.array([])\n",
    "  for raw_record in predictions:\n",
    "      raw_record = numpy.squeeze(raw_record)\n",
    "      rows,cols = raw_record.shape      \n",
    "      raw_record = raw_record[128:384,128:384]\n",
    "      if rowCounter == 1:\n",
    "          finalArray = rowArray\n",
    "      if counter <= patchesPerRow:\n",
    "          if counter == 1:\n",
    "              rowArray = raw_record\n",
    "          else:\n",
    "              rowArray = numpy.concatenate((rowArray,raw_record), axis = 1)\n",
    "          counter = counter+1\n",
    "      else:\n",
    "          counter = 2\n",
    "          rowCounter = rowCounter+1\n",
    "          if numpy.array_equal(finalArray,rowArray):\n",
    "              finalArray = rowArray\n",
    "          else:\n",
    "              finalArray = numpy.concatenate((finalArray,rowArray),axis=0)\n",
    "          rowArray = raw_record #mod de 73 == 0 new line starts\n",
    "      globalCounter = globalCounter+1\n",
    "  finalArray = numpy.concatenate((finalArray,rowArray),axis=0)\n",
    "  show(Image.fromarray(finalArray))\n",
    "  rows,cols = finalArray.shape\n",
    "\n",
    "  driver = gdal.GetDriverByName(\"GTiff\")\n",
    "\n",
    "  finalArray2  = numpy.array([finalArray])\n",
    "  rasterFolder = '/~/local_path/apicum_segmentation/output/v'+VERSION+'/'+MODEL_VERSION+'/'+str(epochs)+'epochs/classifications_tiff/'+str(index_in)\n",
    "\n",
    "  if not os.path.exists(rasterFolder):\n",
    "    print('lets make the directory')\n",
    "    os.makedirs(rasterFolder)\n",
    "  \n",
    "  rasterURI    = rasterFolder + '/UNET_v'+VERSION+'_'+MODEL_VERSION+'_grid_'+str(region_id)+'_year_'+str(index_in)+'_'+str(epochs)+'epochs.tif'\n",
    "  rasterURILZW = rasterFolder + '/outimage_'+VERSION+'_'+MODEL_VERSION+'_'+str(region_id)+'_'+str(index_in)+'_'+str(epochs)+'epochs_lzw.tif'\n",
    " \n",
    "  #print(rasterURILZW)\n",
    "  with rasterio.open(rasterURI,'w',\n",
    "          driver=\"GTiff\",\n",
    "          height=rows,\n",
    "          width=cols,\n",
    "          count=1,\n",
    "          dtype=\"float32\",\n",
    "          crs=mixer[\"projection\"][\"crs\"],\n",
    "          transform=mixer[\"projection\"][\"affine\"][\"doubleMatrix\"],\n",
    "          nodata=\"nan\") as dataset:\n",
    "              dataset.write(finalArray2)\n",
    "  !gdal_translate -of GTiff -co \"COMPRESS=LZW\" -co \"PREDICTOR=2\" -co \"TILED=YES\" {rasterURI} {rasterURILZW}\n",
    "  !rm {rasterURI}\n",
    "    \n",
    "  print('Writing predictions...')\n",
    "  out_image_file  = rasterFolder+'/' + out_image_base + '_'+str(epochs)+'epochs.TFRecord'\n",
    "  out_image_mixer = rasterFolder+'/'+ out_image_base + '_'+str(epochs)+'epochs.json'\n",
    "  #----------------------------------TFRECORD WRITE ----------------------------\n",
    "  writer = tf.io.TFRecordWriter(out_image_file)\n",
    "  patches = 0\n",
    "  for predictionPatch in predictions:\n",
    "    #print('Writing patch ' + str(patches) + '...')\n",
    "    predictionPatch = predictionPatch[\n",
    "        x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
    "\n",
    "    # Create an example.\n",
    "    example = tf.train.Example(\n",
    "      features=tf.train.Features(\n",
    "        feature={\n",
    "          'classification': tf.train.Feature(\n",
    "              float_list=tf.train.FloatList(\n",
    "                  value=predictionPatch.flatten()))\n",
    "        }\n",
    "      )\n",
    "    )\n",
    "    # Write the example.\n",
    "    writer.write(example.SerializeToString())\n",
    "    patches += 1\n",
    "\n",
    "  writer.close()\n",
    "  !cp {jsonFile} {out_image_mixer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "image_base_name = 'allPatch_UNET_grid_'\n",
    "\n",
    "for y in range(1985, 2024):\n",
    "    start = time.time()\n",
    "    print('starting...')\n",
    "    !echo 'year={y}' >> log.log\n",
    "    for region in grid:\n",
    "        region_id = region.properties['id']\n",
    "        region_id = int(region_id)\n",
    "        if region.properties['apicum'] == 1 and not (region_id in processed_grids):\n",
    "            processed_grids.append(region_id)\n",
    "            print(f\"region: {region_id}, year: {y}\")\n",
    "            print('Predicting')\n",
    "            doPrediction(image_base_name+str(region_id)+str('_' + MOSAIC_VERSION),y,region_id, kernel_buffer, region.geometry.coordinates, EPOCH)\n",
    "            print('Finish')\n",
    "            !echo 'grid={y} -{region_id}' >> log.log\n",
    "    end = time.time()\n",
    "    print('Prediction Time per year = '+str(end - start))\n",
    "print('DONE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Geral)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
